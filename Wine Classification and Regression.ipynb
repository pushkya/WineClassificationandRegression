{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "noble-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, AdaBoostRegressor,GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "extraordinary-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('winequality-red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developing-voluntary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dried-demonstration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 7, 4, 8, 3], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cardiovascular-radiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding Correlation\n",
    "Corr=df.corr()\n",
    "Corr_res=[]\n",
    "for i in range(0,len(df.dtypes)):\n",
    "    for j in range(0,len(df.dtypes)):\n",
    "        value=Corr.iloc[i:i+1,j:j+1].values\n",
    "        if value>0.8 and value!=1 :\n",
    "            Corr_res.append(Corr.columns[i])\n",
    "Corr_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sitting-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "df.iloc[:,:-1]=std.fit_transform(df.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "choice-deficit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.528360</td>\n",
       "      <td>0.961877</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>-0.466193</td>\n",
       "      <td>-0.379133</td>\n",
       "      <td>0.558274</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.298547</td>\n",
       "      <td>1.967442</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>0.043416</td>\n",
       "      <td>0.223875</td>\n",
       "      <td>0.872638</td>\n",
       "      <td>0.624363</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>-0.719933</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.298547</td>\n",
       "      <td>1.297065</td>\n",
       "      <td>-1.186070</td>\n",
       "      <td>-0.169427</td>\n",
       "      <td>0.096353</td>\n",
       "      <td>-0.083669</td>\n",
       "      <td>0.229047</td>\n",
       "      <td>0.134264</td>\n",
       "      <td>-0.331177</td>\n",
       "      <td>-0.048089</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.654856</td>\n",
       "      <td>-1.384443</td>\n",
       "      <td>1.484154</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.264960</td>\n",
       "      <td>0.107592</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.664277</td>\n",
       "      <td>-0.979104</td>\n",
       "      <td>-0.461180</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.528360</td>\n",
       "      <td>0.961877</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>-0.466193</td>\n",
       "      <td>-0.379133</td>\n",
       "      <td>0.558274</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>-1.217796</td>\n",
       "      <td>0.403229</td>\n",
       "      <td>-0.980669</td>\n",
       "      <td>-0.382271</td>\n",
       "      <td>0.053845</td>\n",
       "      <td>1.542054</td>\n",
       "      <td>-0.075043</td>\n",
       "      <td>-0.978765</td>\n",
       "      <td>0.899886</td>\n",
       "      <td>-0.461180</td>\n",
       "      <td>0.072294</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>-1.390155</td>\n",
       "      <td>0.123905</td>\n",
       "      <td>-0.877968</td>\n",
       "      <td>-0.240375</td>\n",
       "      <td>-0.541259</td>\n",
       "      <td>2.211469</td>\n",
       "      <td>0.137820</td>\n",
       "      <td>-0.862162</td>\n",
       "      <td>1.353436</td>\n",
       "      <td>0.601055</td>\n",
       "      <td>0.729364</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>-1.160343</td>\n",
       "      <td>-0.099554</td>\n",
       "      <td>-0.723916</td>\n",
       "      <td>-0.169427</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>1.255161</td>\n",
       "      <td>-0.196679</td>\n",
       "      <td>-0.533554</td>\n",
       "      <td>0.705508</td>\n",
       "      <td>0.542042</td>\n",
       "      <td>0.541630</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>-1.390155</td>\n",
       "      <td>0.654620</td>\n",
       "      <td>-0.775267</td>\n",
       "      <td>-0.382271</td>\n",
       "      <td>-0.264960</td>\n",
       "      <td>1.542054</td>\n",
       "      <td>-0.075043</td>\n",
       "      <td>-0.676657</td>\n",
       "      <td>1.677400</td>\n",
       "      <td>0.305990</td>\n",
       "      <td>-0.209308</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>-1.332702</td>\n",
       "      <td>-1.216849</td>\n",
       "      <td>1.021999</td>\n",
       "      <td>0.752894</td>\n",
       "      <td>-0.434990</td>\n",
       "      <td>0.203223</td>\n",
       "      <td>-0.135861</td>\n",
       "      <td>-0.666057</td>\n",
       "      <td>0.511130</td>\n",
       "      <td>0.010924</td>\n",
       "      <td>0.541630</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0         -0.528360          0.961877    -1.391472       -0.453218  -0.243707   \n",
       "1         -0.298547          1.967442    -1.391472        0.043416   0.223875   \n",
       "2         -0.298547          1.297065    -1.186070       -0.169427   0.096353   \n",
       "3          1.654856         -1.384443     1.484154       -0.453218  -0.264960   \n",
       "4         -0.528360          0.961877    -1.391472       -0.453218  -0.243707   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594      -1.217796          0.403229    -0.980669       -0.382271   0.053845   \n",
       "1595      -1.390155          0.123905    -0.877968       -0.240375  -0.541259   \n",
       "1596      -1.160343         -0.099554    -0.723916       -0.169427  -0.243707   \n",
       "1597      -1.390155          0.654620    -0.775267       -0.382271  -0.264960   \n",
       "1598      -1.332702         -1.216849     1.021999        0.752894  -0.434990   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "0               -0.466193             -0.379133  0.558274  1.288643   \n",
       "1                0.872638              0.624363  0.028261 -0.719933   \n",
       "2               -0.083669              0.229047  0.134264 -0.331177   \n",
       "3                0.107592              0.411500  0.664277 -0.979104   \n",
       "4               -0.466193             -0.379133  0.558274  1.288643   \n",
       "...                   ...                   ...       ...       ...   \n",
       "1594             1.542054             -0.075043 -0.978765  0.899886   \n",
       "1595             2.211469              0.137820 -0.862162  1.353436   \n",
       "1596             1.255161             -0.196679 -0.533554  0.705508   \n",
       "1597             1.542054             -0.075043 -0.676657  1.677400   \n",
       "1598             0.203223             -0.135861 -0.666057  0.511130   \n",
       "\n",
       "      sulphates   alcohol  quality  \n",
       "0     -0.579207 -0.960246        5  \n",
       "1      0.128950 -0.584777        5  \n",
       "2     -0.048089 -0.584777        5  \n",
       "3     -0.461180 -0.584777        6  \n",
       "4     -0.579207 -0.960246        5  \n",
       "...         ...       ...      ...  \n",
       "1594  -0.461180  0.072294        5  \n",
       "1595   0.601055  0.729364        6  \n",
       "1596   0.542042  0.541630        6  \n",
       "1597   0.305990 -0.209308        5  \n",
       "1598   0.010924  0.541630        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "finnish-evolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 6, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,-1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "worse-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119, 11)\n",
      "(480, 11)\n",
      "(1119,)\n",
      "(480,)\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, random_state = 40, test_size = 0.3) \n",
    "print(X_train1.shape)\n",
    "print(X_test1.shape)\n",
    "print(y_train1.shape)\n",
    "print(y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "incident-planning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error Rate')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_rate = []\n",
    "for i in range(1, 40):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(X_train1, y_train1)\n",
    "    pred_i = knn.predict(X_test1)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "\n",
    "plt.figure(figsize =(10, 6))\n",
    "plt.plot(range(1, 40), error_rate, color ='blue',linestyle ='dashed', marker ='o',\n",
    "         markerfacecolor ='red', markersize = 10)\n",
    "\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "affiliated-welsh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(metric='manhattan', n_neighbors=12)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#K=12 has minimum error rate\n",
    "model = KNeighborsClassifier(n_neighbors= 12, metric = 'manhattan', p = 2,weights='uniform')\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "executed-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the ouput from input data (x_train) and (y_train) \n",
    "y_pred1 = model.predict(X_train)\n",
    "y_pred2 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "extensive-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of train data set: 0.6419077404222049\n",
      "Accuracy score of test data set: 0.60625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score of train data set:\",accuracy_score(y_train, y_pred1))\n",
    "print(\"Accuracy score of test data set:\",accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "geological-eclipse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1382\n",
       "1     217\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After taking in factor as quality we will divide it in 2 types:0 and 1\n",
    "#0 being Bad and 1 being Good\n",
    "df['quality'] = np.where(df['quality']>6,1,0)\n",
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "clean-caution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assigning dataframe to list of array values\n",
    "X = df.drop(['quality'], axis = 1).values\n",
    "y = df['quality'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "unexpected-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data in the proportion of 70:30 and 86:14\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                   stratify = y,\n",
    "                                                   test_size = 0.3,\n",
    "                                                   random_state = 1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "announced-maker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score 0.8979166666666667 Best K-Score 3\n"
     ]
    }
   ],
   "source": [
    "k = range(1,50,2)\n",
    "testing_accuracy = []\n",
    "training_accuracy = []\n",
    "score = 0\n",
    "#Fitting the model\n",
    "for i in k:\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    pipe_knn = Pipeline([('scale', MinMaxScaler()), ('knn', knn)])\n",
    "    pipe_knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = pipe_knn.predict(X_train)\n",
    "    training_accuracy.append(accuracy_score(y_train, y_pred_train))\n",
    "    \n",
    "    y_pred_test = pipe_knn.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test,y_pred_test)\n",
    "    testing_accuracy.append(acc_score)\n",
    "    \n",
    "    if score < acc_score:\n",
    "        score = acc_score\n",
    "        best_k = i\n",
    "        \n",
    "print('Best Accuracy Score', score, 'Best K-Score', best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "marine-planning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_neighbors\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:34:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best Accuracy Score 0.9125 Best K-Score 1\n"
     ]
    }
   ],
   "source": [
    "k = range(1,50,2)\n",
    "testing_accuracy = []\n",
    "training_accuracy = []\n",
    "score = 0\n",
    "#Fitting the model\n",
    "for i in k:\n",
    "    knn = XGBClassifier(n_neighbors = i)\n",
    "    pipe_knn = Pipeline([('scale', MinMaxScaler()), ('knn', knn)])\n",
    "    pipe_knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = pipe_knn.predict(X_train)\n",
    "    training_accuracy.append(accuracy_score(y_train, y_pred_train))\n",
    "    \n",
    "    y_pred_test = pipe_knn.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test,y_pred_test)\n",
    "    testing_accuracy.append(acc_score)\n",
    "    \n",
    "    if score < acc_score:\n",
    "        score = acc_score\n",
    "        best_k = i\n",
    "        \n",
    "print('Best Accuracy Score', score, 'Best K-Score', best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "correct-dakota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score 0.9208333333333333 Best K-Score 31\n"
     ]
    }
   ],
   "source": [
    "k = range(1,50,2)\n",
    "testing_accuracy = []\n",
    "training_accuracy = []\n",
    "score = 0\n",
    "#Fitting the model\n",
    "for i in k:\n",
    "    randomforest = RandomForestClassifier(n_estimators = i)\n",
    "    pipe_randomforest = Pipeline([('scale', MinMaxScaler()), ('randomforest', randomforest)])\n",
    "    pipe_randomforest.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = pipe_randomforest.predict(X_train)\n",
    "    training_accuracy.append(accuracy_score(y_train, y_pred_train))\n",
    "    \n",
    "    y_pred_test = pipe_randomforest.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test,y_pred_test)\n",
    "    testing_accuracy.append(acc_score)\n",
    "    \n",
    "    if score < acc_score:\n",
    "        score = acc_score\n",
    "        best_k = i\n",
    "        \n",
    "print('Best Accuracy Score', score, 'Best K-Score', best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "regulated-prior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score 0.91875 Best K-Score 9\n"
     ]
    }
   ],
   "source": [
    "k = range(1,50,2)\n",
    "testing_accuracy = []\n",
    "training_accuracy = []\n",
    "score = 0\n",
    "#Fitting the model\n",
    "for i in k:\n",
    "    bag = BaggingClassifier(n_estimators = i)\n",
    "    pipe_bag = Pipeline([('scale', MinMaxScaler()), ('bagging', bag)])\n",
    "    pipe_bag.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = pipe_bag.predict(X_train)\n",
    "    training_accuracy.append(accuracy_score(y_train, y_pred_train))\n",
    "    \n",
    "    y_pred_test = pipe_bag.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test,y_pred_test)\n",
    "    testing_accuracy.append(acc_score)\n",
    "    \n",
    "    if score < acc_score:\n",
    "        score = acc_score\n",
    "        best_k = i\n",
    "        \n",
    "print('Best Accuracy Score', score, 'Best K-Score', best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "proof-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = LinearRegression()\n",
    "linear.fit(X_train1, y_train1)\n",
    "linear_pred = linear.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "seven-regular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear Regression on training set: 0.23\n",
      "Accuracy of Linear Regression on test set: 0.25\n",
      "0.1430977462042151\n",
      "[ 3.82374112e-02 -1.56432774e-01  6.79159299e-02  2.26777057e-02\n",
      " -5.74755365e-01  1.13422960e-04 -8.51000789e-04 -3.49187514e+01\n",
      "  5.76051440e-03  2.84829031e-01  8.22339267e-02] 33.65383068038019\n"
     ]
    }
   ],
   "source": [
    "#Results\n",
    "print('Accuracy of Linear Regression on training set: {:.2f}'.format(linear.score(X_train1, y_train1)))\n",
    "print('Accuracy of Linear Regression on test set: {:.2f}'.format(linear.score(X_test1, y_test1)))\n",
    "accuracy = mean_squared_error(y_test,linear_pred)\n",
    " \n",
    "print(accuracy)\n",
    " \n",
    "weights = linear.coef_\n",
    "intercept = linear.intercept_\n",
    "print(weights,intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "correct-barrel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression on training set: 0.88\n",
      "Accuracy of Logistic Regression on test set: 0.87\n",
      "0.18958333333333333\n",
      "[[-0.03385069 -2.34795963  0.27726676  0.08188676 -1.13864617  0.02745517\n",
      "  -0.02338396 -1.20928944 -2.61633149  1.55497453  0.91011971]] [-1.20029919]\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(solver='liblinear')\n",
    "logistic.fit(X_train1, y_train1)\n",
    "logistic_pred = logistic.predict(X_test1)\n",
    "\n",
    "#Results\n",
    "print('Accuracy of Logistic Regression on training set: {:.2f}'.format(logistic.score(X_train1, y_train1)))\n",
    "print('Accuracy of Logistic Regression on test set: {:.2f}'.format(logistic.score(X_test1, y_test1)))\n",
    "accuracy = mean_squared_error(y_test,logistic_pred)\n",
    " \n",
    "print(accuracy)\n",
    " \n",
    "weights = logistic.coef_\n",
    "intercept = logistic.intercept_\n",
    "print(weights,intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "massive-struggle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM Regression on training set: -0.00\n",
      "Accuracy of SVM Regression on test set: 0.00\n",
      "0.11985419244695202\n"
     ]
    }
   ],
   "source": [
    "svr = SVR()\n",
    "svr.fit(X_train1, y_train1)\n",
    "svr_pred = svr.predict(X_test1)\n",
    "\n",
    "#Results\n",
    "print('Accuracy of SVM Regression on training set: {:.2f}'.format(svr.score(X_train1, y_train1)))\n",
    "print('Accuracy of SVM Regression on test set: {:.2f}'.format(svr.score(X_test1, y_test1)))\n",
    "accuracy = mean_squared_error(y_test,svr_pred)\n",
    " \n",
    "print(accuracy)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "official-series",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RandomForest Regression on training set: 0.91\n",
      "Accuracy of RandomForest Regression on test set: 0.40\n",
      "0.11985419244695202\n"
     ]
    }
   ],
   "source": [
    "rforest = RandomForestRegressor(n_estimators=30)\n",
    "rforest.fit(X_train1, y_train1)\n",
    "rforest_pred = svr.predict(X_test1)\n",
    "\n",
    "#Results\n",
    "print('Accuracy of RandomForest Regression on training set: {:.2f}'.format(rforest.score(X_train1, y_train1)))\n",
    "print('Accuracy of RandomForest Regression on test set: {:.2f}'.format(rforest.score(X_test1, y_test1)))\n",
    "accuracy = mean_squared_error(y_test,rforest_pred)\n",
    " \n",
    "print(accuracy)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "existing-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Adaboost Regression on training set: 0.22\n",
      "Accuracy of Adaboost Regression on test set: 0.23\n",
      "0.15686921589951902\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaBoostRegressor(n_estimators=50, learning_rate=0.8)\n",
    "adaboost.fit(X_train1, y_train1)\n",
    "adaboost_pred = adaboost.predict(X_test1)\n",
    "\n",
    "#Results\n",
    "print('Accuracy of Adaboost Regression on training set: {:.2f}'.format(adaboost.score(X_train1, y_train1)))\n",
    "print('Accuracy of Adaboost Regression on test set: {:.2f}'.format(adaboost.score(X_test1, y_test1)))\n",
    "accuracy = mean_squared_error(y_test,adaboost_pred)\n",
    " \n",
    "print(accuracy)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "enhanced-leave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGboost Regression on training set: 0.98\n",
      "Accuracy of XGboost Regression on test set: 0.40\n",
      "0.1669956608553022\n"
     ]
    }
   ],
   "source": [
    "xgboost = XGBRegressor(n_estimators=120, max_depth=6, eta=0.1, subsample=0.7, colsample_bytree=0.5)\n",
    "xgboost.fit(X_train1, y_train1)\n",
    "xgboost_pred = xgboost.predict(X_test1)\n",
    "\n",
    "#Results\n",
    "print('Accuracy of XGboost Regression on training set: {:.2f}'.format(xgboost.score(X_train1, y_train1)))\n",
    "print('Accuracy of XGboost Regression on test set: {:.2f}'.format(xgboost.score(X_test1, y_test1)))\n",
    "accuracy = mean_squared_error(y_test,xgboost_pred)\n",
    " \n",
    "print(accuracy)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "dried-louis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.820114100513384\n",
      "0.8007217224802109\n",
      "0.20224071928701695\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators=1000, learning_rate = 0.2, max_depth = 6, criterion = 'mse')\n",
    "gbr.fit(X_train1, y_train1)\n",
    "gbr_pred = gbr.predict(X_test1)\n",
    "\n",
    "print(gbr.score(X_train, y_train))\n",
    "print(gbr.score(X_test, y_test))\n",
    "accuracy = mean_squared_error(y_test,gbr_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
